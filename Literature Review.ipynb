{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literature Review\n",
    "\n",
    "\n",
    "    With regard to the literature behind the plan and ultimate execution of this project, we had drawn from a number of research articles as well as Kaggle kernels to shape our approach to predicting home sale prices.  Due to the structure of the dataset, we decided to search for research that would help us understand and optimize Lasso and Ridge manipulation for this competition in particular.  Though they are somewhat similar, Lasso takes Ridge to the next step by performing covariate selection, which ends up making the model far more interpretable.  However, we decided to explore both options and test out to see which one would be more effective in creating a good prediction model.\n",
    "    \n",
    "    We began our research with an article illustrating the use of the Python module “Scikit-learn.”  This article was written by Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay among many others.  The Scikit-learn module is a powerful Python tool that can be used for medium-scale supervised and unsupervised problems in the world of machine learning.  Scikit-learn takes full advantage of the rich Python environment for users to implement machine learning algorithms.  This is an essential tool for our competition, and this article helped us fully understand how to use it.  This article helps us know how to leverage Scikit-learn in the most efficient way.  This serves as the foundation to our model-building and was incredibly important to learn about.\n",
    "    \n",
    "    After reading into the Scikit-learn module, we started looking into research articles to help our understanding of the implementation of Lasso.  The first article we came across regarding Lasso was titled “Stock Market Forecasting Using LASSO Linear Regression Model” written by Sanjiban Sekhar Roy, Dishant Mittal, Avik Basu, and Ajith Abraham.  We not only thought that this article would be useful in learning about real-world uses of Lasso linear regression, but also found the scenario of predicting stock market movements parallel to the problem we are tackling in our project.  Like buying houses, the stock market is ultimately driven by human emotion.  Both the stock market and the real estate market could be forecasted with some degree of accuracy using a very wide range of metrics and statistics, but, in the end, it all comes down to the unpredictable human emotion.  This unpredictability creates many outliers, which is where Lasso shines.\n",
    "    \n",
    "    Due to its strong applicability to this project, we wanted to learn more about Lasso to see if we could deal with our dataset’s long tail.  We came across an article titled “Robust Regression Shrinkage and Consistent Variable Selection Through the LAD-Lasso” by Hansheng Wang, Guodong Li, and Guohua Jiang.  This research article specifically discussed the “least absolute deviation (LAD) regression” and how the regression works with the “least absolute shrinkage and selection operator (Lasso).”  In combining the two, LAD-Lasso can resist heavy-tailed errors and outliers, which is something that we had encountered with our dataset and its inherent skewness.  As it further illustrated the power and use of LAD-Lasso, we noted how we could potentially implement it for our kernel.\n",
    "    \n",
    "    In order to not pigeonhole ourselves and research only one type of regression for the modeling, we also decided to find articles relevant to our other preferred approach: ridge regression.  We first found an article titled “Optimized Parameter Search for Large Datasets of the Regularization Parameter and Feature Selection for Ridge Regression” written by Pieter Buteneers, Ken Caluwaerts, Joni Dambre, David Verstraeten, and Benjamin Schrauwen.  Ridge regression is a very good model for this situation as it can be used when the number of predictors exceeds the number of observations.  This model has a broad range of sale prices which makes this great for ridge regression.  The article dives straight into the heart of ridge regression and it gave us a very good mathematical understanding of how it works. \n",
    "    \n",
    "    Similar to the article with Lasso linear regression being used to predict the stock market, we sought an article that would illustrate how to use ridge regression in real world scenarios.  We found an article literally titled “Ridge Regression in Practice” by Donald W. Marquardt and Ronald D. Snee.  This research article illustrated the practical uses of ridge regression and walked through the foundations and theory of this type of regression.  While the other article on ridge was very technical, this one was watered-down yet extremely practical in how it walked through ridge regression application.  This proved to be incredibly useful for our fundamental understanding and helped us in working with ridge regression with our project’s dataset.\n",
    "    \n",
    "    Along with these research articles, we also took a look through previous kernels on Kaggle.com under the same competition.  We were not as interested in their model building as we were in how they cleaned data.  One of our primary kernels we researched was the one my Pedro Marcelino titled “Comprehensive data exploration with Python” (https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python) where he walks through data exploration and scrubbing in a very basic, easy-to-understand way.  In all our study, we had not focused much at all on data cleaning and have found that many previous kernels are great resources for learning how to effectively scrub and format data to fit into models.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
